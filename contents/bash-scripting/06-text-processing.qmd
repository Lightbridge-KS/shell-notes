---
title: "Module 6: TEXT PROCESSING"
---

## 6.1 grep, sed, awk Basics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE BIG THREE                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   grep    Global Regular Expression Print                               â”‚
â”‚           â”œâ”€â”€ Search for patterns in files                              â”‚
â”‚           â””â”€â”€ Filter lines matching a pattern                           â”‚
â”‚                                                                         â”‚
â”‚   sed     Stream Editor                                                 â”‚
â”‚           â”œâ”€â”€ Transform text (find & replace)                           â”‚
â”‚           â””â”€â”€ Line-by-line processing                                   â”‚
â”‚                                                                         â”‚
â”‚   awk     Pattern scanning and processing                               â”‚
â”‚           â”œâ”€â”€ Column-based data processing                              â”‚
â”‚           â”œâ”€â”€ Built-in variables and functions                          â”‚
â”‚           â””â”€â”€ Mini programming language                                 â”‚
â”‚                                                                         â”‚
â”‚   Quick decision:                                                       â”‚
â”‚   â”œâ”€â”€ Need to FIND lines? â†’ grep                                        â”‚
â”‚   â”œâ”€â”€ Need to REPLACE text? â†’ sed                                       â”‚
â”‚   â””â”€â”€ Need to process COLUMNS? â†’ awk                                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### grep â€” Pattern Searching

```bash
#!/usr/bin/env bash

# Basic search
grep "error" logfile.txt          # Lines containing "error"
grep -i "error" logfile.txt       # Case-insensitive

# Common options
grep -n "error" log.txt           # Show line numbers
grep -c "error" log.txt           # Count matches
grep -l "error" *.log             # List files with matches
grep -L "error" *.log             # List files WITHOUT matches
grep -v "debug" log.txt           # Invert (exclude matches)
grep -w "error" log.txt           # Whole word only
grep -o "error" log.txt           # Only matching part

# Context lines
grep -A 2 "error" log.txt         # 2 lines After match
grep -B 2 "error" log.txt         # 2 lines Before match
grep -C 2 "error" log.txt         # 2 lines Context (before & after)

# Multiple patterns
grep -e "error" -e "warn" log.txt           # Match either
grep -E "error|warn" log.txt                # Extended regex (same)
grep -f patterns.txt log.txt                # Patterns from file

# Recursive search
grep -r "TODO" ./src              # Search in directory
grep -rn "TODO" ./src             # With line numbers
grep -r --include="*.py" "import" ./  # Only .py files

# Quiet mode (for conditionals)
if grep -q "error" log.txt; then
    echo "Errors found!"
fi
```

### sed â€” Stream Editing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    sed SYNTAX                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   sed 'command' file                                                    â”‚
â”‚   sed -e 'cmd1' -e 'cmd2' file    Multiple commands                     â”‚
â”‚   sed -i 'command' file           Edit in place (careful!)              â”‚
â”‚   sed -i.bak 'command' file       Edit in place with backup             â”‚
â”‚                                                                         â”‚
â”‚   Common commands:                                                      â”‚
â”‚   s/old/new/      Substitute (first occurrence per line)                â”‚
â”‚   s/old/new/g     Substitute globally (all occurrences)                 â”‚
â”‚   s/old/new/i     Case-insensitive                                      â”‚
â”‚   d               Delete line                                           â”‚
â”‚   p               Print line                                            â”‚
â”‚   a\text          Append text after                                     â”‚
â”‚   i\text          Insert text before                                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```bash
#!/usr/bin/env bash

# Basic substitution
echo "hello world" | sed 's/world/bash/'     # hello bash
echo "aaa bbb aaa" | sed 's/aaa/xxx/'        # xxx bbb aaa (first only)
echo "aaa bbb aaa" | sed 's/aaa/xxx/g'       # xxx bbb xxx (global)

# Different delimiters (useful for paths)
echo "/usr/local/bin" | sed 's/local/share/'         # Standard
echo "/usr/local/bin" | sed 's|/usr|/opt|'           # Pipe delimiter
echo "/usr/local/bin" | sed 's#/usr#/opt#'           # Hash delimiter

# Delete lines
sed '/^#/d' config.txt            # Delete comment lines
sed '/^$/d' file.txt              # Delete empty lines
sed '1d' file.txt                 # Delete first line
sed '$d' file.txt                 # Delete last line
sed '1,5d' file.txt               # Delete lines 1-5

# Print specific lines
sed -n '5p' file.txt              # Print line 5 only
sed -n '1,10p' file.txt           # Print lines 1-10
sed -n '/error/p' file.txt        # Print matching lines (like grep)

# Insert and append
sed '1i\# Header' file.txt        # Insert at start
sed '$a\# Footer' file.txt        # Append at end
sed '/pattern/a\new line' file    # Append after pattern

# Multiple commands
sed -e 's/foo/bar/g' -e 's/baz/qux/g' file.txt

# Capture groups
echo "hello 123 world" | sed 's/\([0-9]*\)/[\1]/'   # hello [123] world
echo "John Smith" | sed 's/\(.*\) \(.*\)/\2, \1/'  # Smith, John
```

### awk â€” Column Processing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    awk BASICS                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   awk 'pattern { action }' file                                         â”‚
â”‚                                                                         â”‚
â”‚   Built-in Variables:                                                   â”‚
â”‚   $0      Entire line                                                   â”‚
â”‚   $1-$N   Fields (columns) - 1-indexed                                  â”‚
â”‚   NF      Number of fields in current line                              â”‚
â”‚   NR      Current line number (record number)                           â”‚
â”‚   FS      Field separator (default: whitespace)                         â”‚
â”‚   OFS     Output field separator                                        â”‚
â”‚   RS      Record separator (default: newline)                           â”‚
â”‚                                                                         â”‚
â”‚   Patterns:                                                             â”‚
â”‚   BEGIN { }     Run before processing                                   â”‚
â”‚   END { }       Run after processing                                    â”‚
â”‚   /regex/       Lines matching regex                                    â”‚
â”‚   condition     Lines where condition is true                           â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```bash
#!/usr/bin/env bash

# Print specific columns
echo "a b c d" | awk '{print $2}'        # b
echo "a b c d" | awk '{print $1, $3}'    # a c
echo "a b c d" | awk '{print $NF}'       # d (last field)

# Custom delimiter
echo "a:b:c:d" | awk -F: '{print $2}'    # b
echo "a,b,c,d" | awk -F, '{print $1,$3}' # a c

# Print line numbers
awk '{print NR, $0}' file.txt            # Line numbers

# Filter by pattern
awk '/error/' file.txt                   # Lines with "error"
awk '!/debug/' file.txt                  # Lines without "debug"
awk '$3 > 100' data.txt                  # Where column 3 > 100

# Column math
awk '{sum += $1} END {print sum}' numbers.txt       # Sum column 1
awk '{sum += $1} END {print sum/NR}' numbers.txt    # Average

# Format output
awk '{printf "%-10s %5d\n", $1, $2}' data.txt

# BEGIN and END blocks
awk 'BEGIN {print "Name,Score"} {print $1","$2} END {print "Done"}' file

# Multiple conditions
awk '$1 == "alice" && $2 > 90 {print $0}' scores.txt

# Field manipulation
awk '{$3 = $1 + $2; print $0}' data.txt  # Add calculated column

# Change field separator in output
awk 'BEGIN {OFS=","} {print $1,$2,$3}' data.txt
```

### Combining grep, sed, awk

```bash
#!/usr/bin/env bash

# Pipeline example
cat access.log |
    grep "POST" |                    # Filter POST requests
    awk '{print $1, $7}' |           # Extract IP and path
    sed 's|/api/||g' |               # Clean up paths
    sort | uniq -c |                 # Count unique
    sort -rn |                       # Sort by count
    head -10                         # Top 10
```

---

## 6.2 cut, sort, uniq, tr

### cut â€” Extract Columns

```bash
#!/usr/bin/env bash

# By character position
echo "Hello World" | cut -c1-5       # Hello
echo "Hello World" | cut -c7-        # World (7 to end)
echo "Hello World" | cut -c1,7       # HW (chars 1 and 7)

# By field (delimiter-based)
echo "a:b:c:d" | cut -d: -f2         # b
echo "a:b:c:d" | cut -d: -f1,3       # a:c
echo "a:b:c:d" | cut -d: -f2-4       # b:c:d
echo "a,b,c,d" | cut -d, -f2-        # b,c,d (2 to end)

# Complement (all except)
echo "a:b:c:d" | cut -d: -f2 --complement  # a:c:d

# From files
cut -d: -f1,7 /etc/passwd            # Username and shell

# CSV field extraction
cut -d, -f1,3 data.csv               # Columns 1 and 3
```

### sort â€” Sort Lines

```bash
#!/usr/bin/env bash

# Basic sort (alphabetical)
sort file.txt

# Numeric sort
sort -n numbers.txt

# Reverse order
sort -r file.txt
sort -rn numbers.txt                  # Reverse numeric

# Sort by column
sort -t, -k2 data.csv                 # By column 2, comma-delimited
sort -t, -k2n data.csv                # Column 2, numeric
sort -t: -k3,3n /etc/passwd           # By UID

# Multiple sort keys
sort -t, -k1,1 -k2n data.csv          # By col1, then col2 numeric

# Unique sort (remove duplicates)
sort -u file.txt

# Human-readable sizes
du -h * | sort -h                     # Sort by size (K, M, G)

# Random shuffle
sort -R file.txt

# Check if sorted
sort -c file.txt && echo "Already sorted"
```

### uniq â€” Find/Remove Duplicates

```bash
#!/usr/bin/env bash

# IMPORTANT: uniq only works on ADJACENT lines
# Always sort first!

# Remove duplicates
sort file.txt | uniq

# Count occurrences
sort file.txt | uniq -c

# Only duplicates
sort file.txt | uniq -d

# Only unique (non-duplicated)
sort file.txt | uniq -u

# Ignore case
sort file.txt | uniq -i

# Skip first N fields
# Compare from field 2 onward
sort data.txt | uniq -f 1

# Common pattern: count and sort
sort file.txt | uniq -c | sort -rn   # Most common first

# Practical: Find duplicate files by size
find . -type f -exec du -h {} + | sort -k1,1 | uniq -d -w10
```

### tr â€” Translate/Delete Characters

```bash
#!/usr/bin/env bash

# Translate characters
echo "hello" | tr 'a-z' 'A-Z'         # HELLO
echo "HELLO" | tr 'A-Z' 'a-z'         # hello
echo "hello" | tr 'aeiou' '12345'     # h2ll4

# Delete characters
echo "hello 123 world" | tr -d '0-9'  # hello  world
echo "hello   world" | tr -d ' '      # helloworld

# Squeeze repeats
echo "heeelllo" | tr -s 'el'          # helo
echo "a   b   c" | tr -s ' '          # a b c

# Replace characters
echo "hello" | tr 'helo' 'HELO'       # HELLO

# Complement (all except)
echo "hello123" | tr -cd '0-9'        # 123 (delete non-digits)
echo "abc123xyz" | tr -cd 'a-z\n'     # abcxyz

# Remove newlines
tr -d '\n' < file.txt

# Convert Windows to Unix line endings
tr -d '\r' < windows.txt > unix.txt

# Character classes
echo "Hello 123" | tr '[:lower:]' '[:upper:]'   # HELLO 123
echo "Hello 123" | tr -d '[:digit:]'            # Hello

# Character classes available:
# [:alnum:] [:alpha:] [:blank:] [:cntrl:] [:digit:]
# [:graph:] [:lower:] [:print:] [:punct:] [:space:]
# [:upper:] [:xdigit:]
```

---

## 6.3 Regular Expressions in Bash

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REGEX BASICS                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Anchors                                                               â”‚
â”‚   ^        Start of line/string                                         â”‚
â”‚   $        End of line/string                                           â”‚
â”‚                                                                         â”‚
â”‚   Character Classes                                                     â”‚
â”‚   .        Any single character                                         â”‚
â”‚   [abc]    Any of a, b, or c                                            â”‚
â”‚   [^abc]   Not a, b, or c                                               â”‚
â”‚   [a-z]    Range a to z                                                 â”‚
â”‚   \d       Digit (grep -P)         [0-9]                                â”‚
â”‚   \w       Word char (grep -P)     [a-zA-Z0-9_]                         â”‚
â”‚   \s       Whitespace (grep -P)    [ \t\n]                              â”‚
â”‚                                                                         â”‚
â”‚   Quantifiers                                                           â”‚
â”‚   *        Zero or more                                                 â”‚
â”‚   +        One or more (extended regex)                                 â”‚
â”‚   ?        Zero or one (extended regex)                                 â”‚
â”‚   {n}      Exactly n                                                    â”‚
â”‚   {n,}     n or more                                                    â”‚
â”‚   {n,m}    Between n and m                                              â”‚
â”‚                                                                         â”‚
â”‚   Groups & Alternation                                                  â”‚
â”‚   (abc)    Capture group                                                â”‚
â”‚   a|b      a or b (extended regex)                                      â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Basic vs Extended Regex

```bash
#!/usr/bin/env bash

# Basic regex (grep, sed default)
# Need to escape: + ? | { } ( )
grep 'ab*c' file.txt              # a, zero or more b, c
grep 'ab\+c' file.txt             # a, one or more b, c (escaped +)
grep 'ab\?c' file.txt             # a, zero or one b, c (escaped ?)

# Extended regex (grep -E, sed -E, awk)
# No need to escape special chars
grep -E 'ab+c' file.txt           # One or more b
grep -E 'ab?c' file.txt           # Zero or one b
grep -E 'cat|dog' file.txt        # cat or dog
grep -E '^(yes|no)$' file.txt     # Exactly "yes" or "no"
```

### Regex in Bash [[ =~ ]]

```bash
#!/usr/bin/env bash

# Basic pattern matching
string="hello123world"

if [[ "$string" =~ [0-9]+ ]]; then
    echo "Contains numbers"
fi

# Access matched portion
if [[ "$string" =~ ([0-9]+) ]]; then
    echo "Matched: ${BASH_REMATCH[0]}"   # 123 (whole match)
    echo "Group 1: ${BASH_REMATCH[1]}"   # 123 (first group)
fi

# Email validation
email="user@example.com"
pattern='^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$'

if [[ "$email" =~ $pattern ]]; then
    echo "Valid email"
fi

# Parse version string
version="v2.1.3-beta"
if [[ "$version" =~ ^v([0-9]+)\.([0-9]+)\.([0-9]+)(-(.*))?$ ]]; then
    echo "Major: ${BASH_REMATCH[1]}"  # 2
    echo "Minor: ${BASH_REMATCH[2]}"  # 1
    echo "Patch: ${BASH_REMATCH[3]}"  # 3
    echo "Tag: ${BASH_REMATCH[5]}"    # beta
fi

# IP address (basic check)
ip="192.168.1.1"
if [[ "$ip" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
    echo "Looks like an IP"
fi
```

### Practical Regex Examples

```bash
#!/usr/bin/env bash

# Find all email addresses in file
grep -oE '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}' file.txt

# Find URLs
grep -oE 'https?://[^[:space:]]+' file.txt

# Find IP addresses
grep -oE '\b[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\b' log.txt

# Find dates (YYYY-MM-DD)
grep -oE '[0-9]{4}-[0-9]{2}-[0-9]{2}' file.txt

# Validate phone numbers
phone="123-456-7890"
if [[ "$phone" =~ ^[0-9]{3}-[0-9]{3}-[0-9]{4}$ ]]; then
    echo "Valid phone format"
fi

# Extract specific parts with sed
# Get domain from URLs
echo "https://www.example.com/path" | sed -E 's|https?://([^/]+).*|\1|'

# Remove HTML tags
echo "<p>Hello <b>World</b></p>" | sed -E 's/<[^>]+>//g'

# Extract quoted strings
echo 'name="Alice" age="30"' | grep -oE '"[^"]+"'
```

---

## 6.4 Processing CSV/TSV Files

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CSV PROCESSING TOOLS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Simple cases (no quoted fields with commas):                          â”‚
â”‚   â”œâ”€â”€ cut -d,      Extract columns                                      â”‚
â”‚   â”œâ”€â”€ awk -F,      Process with logic                                   â”‚
â”‚   â””â”€â”€ while IFS=,  Read into variables                                  â”‚
â”‚                                                                         â”‚
â”‚   Complex cases (quoted fields, embedded commas):                       â”‚
â”‚   â”œâ”€â”€ csvtool       Linux package                                       â”‚
â”‚   â”œâ”€â”€ csvkit (Python)                                                   â”‚
â”‚   â””â”€â”€ miller (mlr)  Advanced CSV/JSON processor                         â”‚
â”‚                                                                         â”‚
â”‚   WARNING: Simple bash parsing breaks on:                               â”‚
â”‚   â”œâ”€â”€ "Field, with comma"                                               â”‚
â”‚   â”œâ”€â”€ Fields with newlines                                              â”‚
â”‚   â””â”€â”€ Escaped quotes ""                                                 â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Simple CSV Processing

```bash
#!/usr/bin/env bash

# Sample CSV (data.csv):
# name,age,city
# Alice,30,New York
# Bob,25,Los Angeles
# Carol,35,Chicago

# Extract columns
cut -d, -f1 data.csv              # Names only
cut -d, -f1,3 data.csv            # Names and cities

# Skip header
tail -n +2 data.csv | cut -d, -f1

# Filter with awk
awk -F, '$2 > 28 {print $1}' data.csv          # Names where age > 28
awk -F, 'NR > 1 && $2 > 28' data.csv           # Skip header, age > 28

# Sum a column (skip header)
awk -F, 'NR > 1 {sum += $2} END {print sum}' data.csv

# Read CSV into variables
while IFS=, read -r name age city; do
    echo "$name is $age years old and lives in $city"
done < <(tail -n +2 data.csv)

# Create CSV from data
{
    echo "name,value,date"
    echo "Alice,100,2024-01-15"
    echo "Bob,200,2024-01-16"
} > output.csv
```

### Transform CSV Data

```bash
#!/usr/bin/env bash

# Convert CSV to TSV
sed 's/,/\t/g' data.csv > data.tsv

# Convert TSV to CSV
sed 's/\t/,/g' data.tsv > data.csv

# Add a column
awk -F, 'BEGIN {OFS=","} NR==1 {print $0,"new_col"} NR>1 {print $0,"value"}' data.csv

# Reorder columns
awk -F, 'BEGIN {OFS=","} {print $3,$1,$2}' data.csv

# Filter and transform
awk -F, 'BEGIN {OFS=","} NR==1 || $2 > 30 {print $1, $2*12, $3}' data.csv
# Output annual age (months) for people over 30

# Merge two CSVs (simple join by line)
paste -d, file1.csv file2.csv

# Remove specific column (e.g., column 2)
cut -d, -f1,3- data.csv
```

### Handling TSV Files

```bash
#!/usr/bin/env bash

# TSV is often easier to process (no quoting issues)

# Create TSV
printf "name\tage\tcity\n" > data.tsv
printf "Alice\t30\tNew York\n" >> data.tsv
printf "Bob\t25\tLos Angeles\n" >> data.tsv

# Process TSV with awk
awk -F'\t' '{print $1, $3}' data.tsv

# Read TSV line by line
while IFS=$'\t' read -r name age city; do
    echo "$name ($age): $city"
done < data.tsv
```

### Advanced Processing with Column Calculation

```bash
#!/usr/bin/env bash

# Calculate new columns from existing
# input: product,price,quantity
# output: product,price,quantity,total

awk -F, 'BEGIN {OFS=","}
    NR == 1 {print $0, "total"; next}
    {print $0, $2 * $3}
' sales.csv

# Group by and aggregate
# Count entries per city
awk -F, 'NR > 1 {cities[$3]++}
    END {for (city in cities) print city, cities[city]}
' data.csv

# Sum by category
awk -F, 'NR > 1 {sum[$1] += $2}
    END {for (cat in sum) print cat, sum[cat]}
' transactions.csv

# Calculate statistics
awk -F, 'NR > 1 {
    sum += $2
    count++
    if (NR == 2 || $2 < min) min = $2
    if (NR == 2 || $2 > max) max = $2
}
END {
    print "Count:", count
    print "Sum:", sum
    print "Average:", sum/count
    print "Min:", min
    print "Max:", max
}' data.csv
```

### Complete CSV Pipeline Example

```bash
#!/usr/bin/env bash

# Analyze sales data
# File: sales.csv
# date,product,category,amount

# Top 5 products by total sales
tail -n +2 sales.csv |
    awk -F, '{sales[$2] += $4} END {for (p in sales) print sales[p], p}' |
    sort -rn |
    head -5

# Daily totals
tail -n +2 sales.csv |
    awk -F, '{daily[$1] += $4} END {for (d in daily) print d, daily[d]}' |
    sort

# Category breakdown
tail -n +2 sales.csv |
    cut -d, -f3,4 |
    awk -F, '{cat[$1] += $2; count[$1]++}
        END {for (c in cat) printf "%s: $%.2f (%d items)\n", c, cat[c], count[c]}'
```

---

## ğŸ§ª Practice Exercises

### Exercise 1: Log Analyzer
Given a web server log file, extract:
1. Top 10 most accessed URLs
2. Count of requests per HTTP status code
3. Total requests from each unique IP

### Exercise 2: CSV Report Generator
Given a CSV with `name,department,salary`:
1. Calculate average salary by department
2. Find employees above average salary
3. Output sorted by department, then salary

### Exercise 3: Text Cleaner
Create a script that:
1. Converts text to lowercase
2. Removes extra whitespace
3. Removes non-alphanumeric characters (keep spaces)
4. Outputs cleaned text

---

## Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 TEXT PROCESSING CHEAT SHEET                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  grep OPTIONS                                                           â”‚
â”‚  -i  ignore case        -v  invert match      -n  line numbers          â”‚
â”‚  -c  count              -l  list files        -o  only matching         â”‚
â”‚  -E  extended regex     -r  recursive         -q  quiet (for if)        â”‚
â”‚  -A/-B/-C n  context lines                                              â”‚
â”‚                                                                         â”‚
â”‚  sed COMMANDS                                                           â”‚
â”‚  s/old/new/     substitute first      s/old/new/g   substitute all      â”‚
â”‚  /pat/d         delete matching       /pat/p        print matching      â”‚
â”‚  -i             edit in place         -E            extended regex      â”‚
â”‚                                                                         â”‚
â”‚  awk BASICS                                                             â”‚
â”‚  $1,$2,...      fields                $0     entire line                â”‚
â”‚  NR             line number           NF     number of fields           â”‚
â”‚  -F','          field separator       BEGIN/END blocks                  â”‚
â”‚  {print $1}     action                /pat/ {action}  pattern matching  â”‚
â”‚                                                                         â”‚
â”‚  OTHER TOOLS                                                            â”‚
â”‚  cut -d: -f1    extract field         sort -k2 -n    sort by col 2      â”‚
â”‚  sort -u        unique sort           uniq -c        count duplicates   â”‚
â”‚  tr 'a-z' 'A-Z' translate             tr -d '0-9'    delete digits      â”‚
â”‚                                                                         â”‚
â”‚  COMMON PATTERNS                                                        â”‚
â”‚  sort | uniq -c | sort -rn            count and rank                    â”‚
â”‚  grep -oE 'pattern'                   extract all matches               â”‚
â”‚  awk -F, '$2>10'                      filter CSV by column              â”‚
â”‚  sed 's/^/prefix/'                    add prefix to lines               â”‚
â”‚  tr -s ' '                            squeeze spaces                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
